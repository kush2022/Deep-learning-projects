{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b68e560",
   "metadata": {},
   "source": [
    "# Deep Learning \n",
    "- is a subset of machine learning that focuses on training artificial neural networks to perform tasks like image and speech recognition, natural language processing and more. \n",
    "- These neural networks are inspired by the human brain, consisting of interconnected nodes organized into layers.\n",
    "- 'Deep' refers to the multiple layers through which data is processed to extract features and patterns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e662c6",
   "metadata": {},
   "source": [
    "### Key Concepts in Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e643414",
   "metadata": {},
   "source": [
    "1. **Neural Networks**\n",
    "- These are fundermental building blocks of deep learning. \n",
    "- Neural networks consists of an input layer, one or more hidden layer, and an output layer. \n",
    "\n",
    "2. **Activation Functions**\n",
    "- introduces non-linearity to neural networks, enabling them to learn complex relationships in data \n",
    "- common activation functions include ReLu, sigmoid, tanh \n",
    "\n",
    "3. **Loss Functions**\n",
    "- measures the difference between predicted values and actual values. \n",
    "- the choice of loss function depends on the task eg. MSE is used for regression, while categorical cross-entropy is used for classification \n",
    "\n",
    "4. **Optimizatin Algorithms**\n",
    "- these algorithms updates the neural network's weight and bias to minimize the loss function. \n",
    "- popular optimizers include Adam, stochastic, batch gradient descent \n",
    "\n",
    "5. **Backpropagation**\n",
    "- is a key training technique in deep learning.\n",
    "- it calculates the gradient of the loss function with respect to the model's parameters, allowing the model to adjust its weight and biases to minimize the loss.\n",
    "\n",
    "6. **Overfitting and Regularization**\n",
    "- overfitting occurs when a model performs on training data but poorly on unseen data. \n",
    "- regularization techniques like dropout and L2 regularization help prevent overfitting by controlling the complixity of the model.\n",
    "\n",
    "7. **Convolutional Neural Network (CNN)**\n",
    "- Are designed for image processsing tasks. \n",
    "- they use convolutional layers to automatically learn features from images.\n",
    "\n",
    "8. **Recurrent Neural Networks (KNN)**\n",
    "- are used for sequences of data, such as time series or natural language.\n",
    "- they have loops to allow information to be passed from one step of the sequence to the next\n",
    "\n",
    "9. **Transfer Learning**\n",
    "- involves using pre-trained model on a related task as a starting point on the new task.\n",
    "- this can be signifcantly speed up training and improve performance, especially when the data is limited.\n",
    "\n",
    "10. **Generative Adversarial Network(GaN)**\n",
    "- consists of two neural networks, a generator and a discriminator, that work together to generate realistic data. \n",
    "- They are used for tasks like image generation and style transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a96de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
