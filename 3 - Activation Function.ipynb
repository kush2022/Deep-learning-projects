{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00cc5cfc",
   "metadata": {},
   "source": [
    "## Activation Function \n",
    "- activation function in a neural network determines the output of a neuron given a set of inputs. \n",
    "- it introduces non-linearity to the network, enabling it to learn complex relationships and patterns in data. \n",
    "- a neural network without an activation function is essentially just a linear regression model. \n",
    "- thus we use non linear tranformation to the inputs of the neuron and this non-linearity in the network is introduced by an activation function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8b7425",
   "metadata": {},
   "source": [
    "## 1. Binary Step Function\n",
    "- the first thing that comes to our minds when we have an activation function would be a threshold base classifier i.e whether or not the neuron should be activated based on the value from the linear transformation \n",
    ">\n",
    "f(x) = 1, x>=0\n",
    "     = 0, x<0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3feae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_step(x):\n",
    "    if x < 0:\n",
    "        return 0 \n",
    "    else:\n",
    "        return 1 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67a3a4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_step(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52144634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_step(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d46dc",
   "metadata": {},
   "source": [
    "## 2. Sigmoid function \n",
    "- The sigmoid activation squashes the input into the range between 0 and 1. \n",
    "- it is often used in the output layer of binary classification problems.\n",
    "- however if suffers from vanishing gradient problems, limiting its effectiveness\n",
    ">\n",
    "f(x) = 1/(1+e^-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5de54b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def sigmoid_func(x):\n",
    "    z = (1 / (1 + np.exp(-x)))\n",
    "    \n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87553f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_func(212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c88c57f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0261879630648827e-10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_func(-23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10c910c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7894680920908113e-10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_func(-22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f85eb3",
   "metadata": {},
   "source": [
    "## 3. Tanh \n",
    "- is a function similar to sigmoid function. \n",
    "- the only difference is that it is symmetric around the origin.\n",
    "- the range of values in this case is from -1 to 1. \n",
    ">\n",
    "tanh(x)=2sigmoid(2x)-1\n",
    ">\n",
    "tanh(x) = 2/(1+e^(-2x)) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb1b5ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_function(x):\n",
    "    z = (2 / (1 +np.exp(-2*x))) -1\n",
    "    \n",
    "    return z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f078eb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.379948962255225"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_function(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6a6ec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7615941559557649"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_function(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2795d",
   "metadata": {},
   "source": [
    ">> usually tanh is preferred over the sigmoid function since it is zero centered and the gradients are not restricted to move in a certain direction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ca53c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e06f4d26",
   "metadata": {},
   "source": [
    "## 4. ReLu \n",
    "- Rectified Linear Activation \n",
    "- it is the most widely used activation function today. \n",
    "- it outputs the input if it's positive and zero otherwise.\n",
    "- it doesn't suffer from vanishing gradient issues for positive inputs, making it a good choice for training deep networks.\n",
    ">\n",
    "f(x) = max(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "524bfefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_function(x):\n",
    "    return max(0, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "486893b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_function(24\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2761204d",
   "metadata": {},
   "source": [
    ">  if you look at the negative side of the graph, you will notice that the gradient value is zero. \n",
    "> DUe to this reason, during backpropogation process, the weights and biases for some neurons are not updated.\n",
    "> This can create dead neuronds which never get activated. This is taken care of by Leaky ReLU function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119ba3aa",
   "metadata": {},
   "source": [
    "## 5. Leaky ReLU\n",
    "- is a variation of ReLU that allows a small, non-zero gradient for negative inputs, mitigaing the dead neuron problems\n",
    ">\n",
    "f(x)= 0.01x, x<0\n",
    "    =   x, x>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e9eac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x):\n",
    "    if x < 0:\n",
    "        return 0.01 * x \n",
    "    else:\n",
    "        return x \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a3d24d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_relu(343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1165145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-24.240000000000002"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_relu(-2424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b49db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78f8800f",
   "metadata": {},
   "source": [
    "## 6. Swish \n",
    "- is a lesser known activation function which was discovered by researchers at Google. \n",
    "- is a computationally efficient as ReLU and shows better performance than ReLU on deeper models. \n",
    "- the values for swish ranges from negative infinity to infinity.\n",
    ">\n",
    "f(x) = x*sigmoid(x)\n",
    "f(x) = x/(1-e^-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f03c8a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish_function(x):\n",
    "    z = x / (x / (1 + np.exp(-x)))\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5433174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swish_function(234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8df5eacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2160792462083295e+101"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swish_function(-234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3f594",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Softmax \n",
    "- is often described as a combination of multiple sigmoids. \n",
    "- it can be used for multiclass classification problems. \n",
    ">\n",
    "softmax(x_i) = exp(x_i) / sum(exp(x_j) for j in all classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcce2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_function(x):\n",
    "    z = np.exp(x)\n",
    "    z_ = z/z.sum()\n",
    "    \n",
    "    return z_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff96b52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08021815, 0.11967141, 0.80011044])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_function([0.8, 1.2, 3.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f09a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
