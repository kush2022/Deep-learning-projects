{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e04a406",
   "metadata": {},
   "source": [
    "## Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d955d39",
   "metadata": {},
   "source": [
    "> Is a fundermental optimazation algorithm used in machine learning and deep learning to minimize the loss or cost function of a model. \n",
    "> The goal is to iteratively adjust the model's parameters ( weights and biases) to find the values that minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5805e64b",
   "metadata": {},
   "source": [
    "### Here's how it works\n",
    "## 1. _Initialization__\n",
    "- Start with the initial guess for the model's parameters. These parameters are often initialized randomly or with some predefined values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1f5940",
   "metadata": {},
   "source": [
    "### 2.  _Calculate the Gradient__\n",
    "- Compute the gradient of the loss function with respect to each model paramter. The gradient points in the direction of the steepest increase in the loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c7ef8",
   "metadata": {},
   "source": [
    "### 3. _Updated the Parameters_\n",
    "- Adjust the model's parameters by taking a step in the opposite direction of the gradient. This step size is known as the learning rate \n",
    "- New Parameter = Old Parameter - (Learning Rate * Gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf5f46d",
   "metadata": {},
   "source": [
    "### 4. _Repeat_\n",
    "- Continue step 2 and 3 iteratively until a stopping criterion is met. \n",
    "- This criterion is often a maximum number of iterations or when the change in the loss function becomes very small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9830d",
   "metadata": {},
   "source": [
    "### 5. _Convergence_\n",
    "- Ideally, the algorithm will converge to a minimum of the loss function, where the gradient is close to zero. This represents the best set of model parameters for the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c58227",
   "metadata": {},
   "source": [
    "## Types of Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd4ddc2",
   "metadata": {},
   "source": [
    "### 1. *Batch Gradient Descent*\n",
    "- In batch gradient descent, the entire training dataset is used to compute the gradient of the loss function at each iteration. It provides a precise estimate of the gradient but can be computationall expensive for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef009dcf",
   "metadata": {},
   "source": [
    "### 2. *Stochastic Gradient Descent SGD*\n",
    "- In SGD, only random data point( or a small batch of data points) is used to compute the gradient at each iteration. This approach is faster but can have noisy updates. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef0dcc2",
   "metadata": {},
   "source": [
    "### 3. *Mini-Batch Gradient*\n",
    "- This strikes a balance between batch gradient descent and SGD. It uses a small, randomly selected subset of the training data at each iteration. This is the most commonly used variant in practice, as it combines efficieny with stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e886c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
