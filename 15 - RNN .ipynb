{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afdca615",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network \n",
    "- is a type of artificial neural network designed for tasks that involve seqential data, such as time series prediction, natural language processing, speech recognition, and more. \n",
    "- RNNs are specifically structured to handle input data with temporal dependencies, where the order and context of data points matter. \n",
    "\n",
    "\n",
    "**Key Features**\n",
    "1. **Recurrent Connections** \n",
    "- RNNs ave recurrent connections that allow information to flow in loops within the network. This looped structure enables them to maintain a memory of previous time steps, making them suitable for sequential data processing \n",
    "\n",
    "\n",
    "2. **Hidden State**\n",
    "- At each time step, an RNN maintains a hidden state vector, which capture information from previous time steps. This hidden state serves as the memory of the network and influences the predictions at the current time step.\n",
    "\n",
    "3. **Share Parameters**\n",
    "- RNNs use the same set of parameters (weights and biases) for each time step in the sequence. \n",
    "- This share structure allows the network to learn and generalize  across time steps.\n",
    "\n",
    "4. **Vanishing Gradient Problem**\n",
    "- Traditional RNNs are susceptible to the vanishing gradient problem, which makes it challenging for them to capture long-range dependencies in data. As a result, they oftem struggle with learning dependencies that span many time steps.\n",
    "\n",
    "\n",
    "\n",
    "_To address some of the limitations of traditional RNNs, several advanced variants have been developed.\n",
    "\n",
    "**Long Short-Term Memory(LSTM)**\n",
    "- LSTMs are a type of RNN that includes specialized gating mechanisms (input, forget, and output gates) to better capture and manage long-range dependencies.  \n",
    "- LSTMs are less prone to the vanishing gradient problem and can store information over extended sequences.\n",
    "\n",
    "**Gated Recurrent Unit(GRU)**\n",
    "- GRUs are another variant of RNNs with gating mechanism similar to LSTMs but with a simplified architecture. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
